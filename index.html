<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
		<!-- Replace the content tag with appropriate information -->
		<meta name="description"
			content="OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?">
		<meta property="og:title" content="OVO-Bench" />
		<meta property="og:description"
			content="OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?" />
		<meta property="og:url" content="https://joeleelyf.github.io/OVO-Bench/" />
		<!-- Keywords for your paper to be indexed by-->
		<meta name="keywords" content="online;video;benchmark">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<title>OVO-Bench</title>

		<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
		<link rel="icon" type="image/x-icon" href="static/images/logo.png">
		<link rel="stylesheet" href="static/css/bulma.min.css">
		<link rel="stylesheet" href="static/css/bulma-carousel.min.css">
		<link rel="stylesheet" href="static/css/bulma-slider.min.css">
		<link rel="stylesheet" href="static/css/fontawesome.all.min.css">
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
		<link rel="stylesheet" href="static/css/index.css">

		<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
		<script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
		<script defer src="static/js/fontawesome.all.min.js"></script>
		<script src="static/js/bulma-carousel.min.js"></script>
		<script src="static/js/bulma-slider.min.js"></script>
		<script src="static/js/index.js"></script>
	</head>
	<body>

		<section class="hero">
			<div class="hero-body">
				<div class="container is-max-desktop">
					<div class="columns is-centered">
						<div class="column has-text-centered">
							<h1 class="title is-1 publication-title"><b style="color: #0088cc;">O</b><b style="color: #060270;">V</b><b style="color: #0088cc;">O</b>-Bench:</h1>
							<h2 class="subtitle is-3 publication-subtitle">
								How Far is Your Video-LLMs from Real-World <b style="color: #0088cc;">O</b>nline <b
									style="color: #060270;">V</b>ide<b style="color: #0088cc;">O</b> Understanding?
							</h2>
							<div class="is-size-5 publication-authors">
								<!-- Paper authors -->
								<span class="author-block">
									<a href="https://scholar.google.com/citations?user=AkjeQ14AAAAJ&hl=en" target="_blank">Yifei Li</a><sup>*1,2&dagger;</sup>,</span>
								<span class="author-block">
									<a href="https://scholar.google.com/citations?hl=en&user=MIt0UGUAAAAJ" target="_blank">Junbo Niu</a><sup>*1,3&dagger;</sup>,</span>
								<span class="author-block">
									<a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Ziyang Miao</a><sup>3</sup>,</span>
								</span>
								<span class="author-block">
									<a href="https://john-ge.github.io/" target="_blank">Chunjiang Ge</a><sup>2</sup>,</span>
								</span>
								<span class="author-block">
									<a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yuanhang Zhou</a><sup>2</sup>,</span>
								</span>
								<span class="author-block">
									<a href="https://github.com/aQiaoz" target="_blank">Qihao He</a><sup>4</sup>
								</span>
							</div>
							<div class="is-size-5 publication-authors">
								<span class="author-block">
									<a href="https://scholar.google.com/citations?hl=en&user=FscToE0AAAAJ" target="_blank">Xiaoyi Dong</a><sup>1,5</sup>,</span>
								</span>
								<span class="author-block">
									<a href="https://scholar.google.com/citations?hl=en&user=vi3W-m8AAAAJ" target="_blank">Haodong Duan</a><sup>1</sup>,</span>
								</span>
								<span class="author-block">
									<a href="https://scholar.google.com/citations?hl=en&user=jmFZZYMAAAAJ" target="_blank">Shuangrui Ding</a><sup>1,5&dagger;</sup>,</span>
								</span>
								<span class="author-block">
									<a href="https://scholar.google.com/citations?hl=en&user=QehSWiQAAAAJ" target="_blank">Rui Qian</a><sup>*1,5&dagger;</sup>
								</span>
							</div>
							<div class="is-size-5 publication-authors">
								<span class="author-block">
									<a href="https://scholar.google.com/citations?hl=en&user=moHH480AAAAJ" target="_blank">Pan Zhang</a><sup>1</sup>,</span>
								</span>
								<span class="author-block">
									<a href="https://scholar.google.com/citations?hl=en&user=hW23VKIAAAAJ" target="_blank">Yuhang Zang</a><sup>1</sup>,</span>
								</span>
								<span class="author-block">
									<a href="https://scholar.google.com/citations?hl=en&user=sJkqsqkAAAAJ" target="_blank">Yuhang Cao</a><sup>1</sup>,</span>
								</span>
								<span class="author-block">
									<a href="https://scholar.google.com/citations?hl=en&user=PopTv7kAAAAJ" target="_blank">Conghui He</a><sup>6</sup>,</span>
								</span>
								<span class="author-block">
									<a href="https://scholar.google.com/citations?user=GDvt570AAAAJ&hl=en" target="_blank">Jiaqi Wang</a><sup>1</sup>
								</span>
							</div>

							<div class="is-size-5 publication-authors">
								<span class="author-block"><sup>1</sup>Shanghai AI Laboratory,</span>
								<span class="author-block"><sup>2</sup>Tsinghua University</span>
							</div>
							<div class="is-size-5 publication-authors">
								<span class="author-block"><sup>3</sup>Beihang University,</span>
								<span class="author-block"><sup>4</sup>Communication University of China</span>
							</div>
							<div class="is-size-5 publication-authors">
								<span class="author-block"><sup>5</sup>The Chinese University of Hong Kong,</span>
								<span class="author-block"><sup>6</sup>SenseTime Group</span>
								<span class="eql-cntrb"><small><br><sup>*</sup>indicates
										Equal Contribution; &dagger; indicates interns at IXCLab, Shanghai AI Laboratory</small></span>
							</div>

							<div class="column has-text-centered">
								<div class="publication-links">
									<!-- Arxiv PDF link -->
									<!-- <span class="link-block">
										<a href="static/pdfs/paper.pdf" target="_blank"
											class="external-link button is-normal is-rounded is-dark">
											<span class="icon">
												<i class="fas fa-file-pdf"></i>
											</span>
											<span>Paper</span>
										</a>
									</span> -->

									<!-- Supplementary PDF link -->
									<!-- <span class="link-block">
										<a href="static/pdfs/supplementary.pdf" target="_blank"
											class="external-link button is-normal is-rounded is-dark">
											<span class="icon">
												<i class="fas fa-file-pdf"></i>
											</span>
											<span>Supplementary</span>
										</a>
									</span> -->

									<!-- Github link -->
									<span class="link-block">
										<a href="https://github.com/JoeLeelyf/OVO-Bench" target="_blank"
											class="external-link button is-normal is-rounded is-dark">
											<span class="icon">
												<i class="fab fa-github"></i>
											</span>
											<span>Code</span>
										</a>
									</span>

									<!-- ArXiv abstract Link -->
									<span class="link-block">
										<a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
											class="external-link button is-normal is-rounded is-dark">
											<span class="icon">
												<i class="ai ai-arxiv"></i>
											</span>
											<span>arXiv</span>
										</a>
									</span>

									<!-- Huggingface Link -->
									<span class="link-block">
										<a href="https://huggingface.co/datasets/JoeLeelyf/OVO-Bench" target="_blank"
											class="external-link button is-normal is-rounded is-dark">
											<span class="icon">
												<p style="font-size:18px">
													ðŸ¤—
												</p>
											</span>
											<span>Dataset</span>
										</a>
									</span>
								</div>
							</div>
						</div>
					</div>
				</div>
			</div>
		</section>

		<!-- Paper abstract -->
		<section class="section hero is-centered">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-four-fifths">
						<h2 class="title is-3">Abstract</h2>
						<div class="content has-text-justified">
							<p>
								Temporal Awarenessâ€”the ability to reason dynamically based on the timestamp when a question is raisedâ€”is the key distinction between offline and online video LLMs. Unlike offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally and dynamically adapt their responses based on the timestamp at which the question is posed. Despite its significance, temporal awareness has not been adequately evaluated in existing benchmarks.
								To fill this gap, we present <b>OVO-Bench</b> (<b>O</b>nline-<b>V</b>ide<b>O</b>-<b>Bench</b>mark), a novel video benchmark that emphasizes the importance of timestamps for advanced online video understanding capability benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and respond to events occurring at specific timestamps under three distinct scenarios: (1) <b>Backward tracing</b>: trace back to past events to answer the question. (2) <b>Real-time understanding</b>: understand and respond to events as they unfold at the current timestamp. (3) <b>Forward active responding</b>: delay the response until sufficient future information becomes available to answer the question accurately.
								OVO-Bench comprises 12 tasks, featuring 644 unique videos and approximately human-curated 2,800 fine-grained meta-annotations with precise timestamps. We combine automated generation pipelines with human curation. With these high-quality samples, we further developed an evaluation pipeline to systematically query video LLMs along the video timeline.
								Evaluations of nine Video-LLMs reveal that, despite advancements on traditional benchmarks, current models struggle with online video understanding, showing a significant gap compared to human agents. We hope OVO-Bench will drive progress in video LLMs and inspire future research in online video reasoning. Our benchmark and code can be accessed at <a href="https://github.com/JoeLeelyf/OVO-Bench">https://github.com/JoeLeelyf/OVO-Bench</a>.
							</p>
						</div>
					</div>
				</div>
			</div>
		</section>
		<!-- End paper abstract -->

		<!-- Taxnomy -->
		<section class="section hero is-light">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-full">
						<h2 class="title is-3">Task Taxnomy of OVO-Bench</h2>
					</div>
				</div>
			</div>
		</section>
		<section class="section hero is-centered">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-four-fifths">
						<div class="content has-text-justified">
							<br> Online video understanding aims to equip real-world,
							always-on agents with the ability to receive and process
							video inputs continuously. We closely mimic human's visual understanding process, 
							which can be depicted as below:
							<br><img src="static/images/online_thinking_taxnomy.png">
							<br> Tasks of different modes are as follows:
							<ol>
								<li> <b>Backward Tracing</b> </li>
								<ul>
									<li> <b>[EPM] Episodic Memory</b>:  Backtrack and retrieve key moments from past video inputs.</li>
									<li> <b>[ASI] Action Sequency Identification</b>: Identify the correct sequence of human actions in the video streams. </li>
									<li> <b>[HLD] Hallucination Detection</b>: Ask questions irrelevant to existing video inputs. </li>
								</ul>
								<li> <b>Real-Time Visual Perception</b> </li>
								<ul>
									<li> <b>[STU] Spatial Understanding</b>: Reason over the spatial relationships between objects occuring in nearby frames. </li>
									<li> <b>[OJR] Object Recognition</b>: Recognize the objects appearing in the current frames. </li>
									<li> <b>[ATR] Attribute Recognition</b>: Identify the characteristics or properties of objects. </li>
									<li> <b>[ACR] Action Recognition</b>: Recognize and interpret the actions being performed by individuals in current frames. </li>
									<li> <b>[OCR] Optical Character Recognition</b>: Recognize and interpret characters that appear within the frame. </li>
									<li> <b>[FTP] Future Prediction</b>: Forecast the most probable subsequent phase of the current scene. </li>
								</ul>
								<li> <b>Forward Active Responding</b> </li>
								<ul>
									<li> <b>[REC] Repetition Event Count</b>: Respond when a repetitive event occurs again. </li>
									<li> <b>[SSR] Sequential Steps Recognition</b>: Respond when a certain procedure or sequence of actions has transitioned to another stage. </li>
									<li> <b>[CRR] Clues Reveal Responding</b>: Delay responding until sufficient information or clues are provided. </li>
								</ul>
							</ol>
						</div>
					</div>
				</div>
			</div>
		</section>
		<!-- Taxnomy -->

		<!-- Comparision -->
		<section class="section hero is-light">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-full">
						<h2 class="title is-3">OVO-Bench Statistics Analysis</h2>
					</div>
				</div>
			</div>
		</section>
		<section class="section hero is-centered">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-full">
						<img src="static/images/statistics_analysis.png">
					</div>
				</div>
				<div class="columns is-centered has-text-centered">
					<div class="content has-text-centered">
						<b>OVO-Bench: benchmark designed for online visual understanding.</b> 
						(a) For each video, we densely query Video-LLMs along the video stream to simulate online conversation scene.
						(b) Our benchmarks features a varied duration of query timestamps and video durations.
						(c) We include a large propotion of ego-centric video in our data source.
					</div>
				</div>
			</div>
		</section>

		<!-- Comparision -->

		<!-- Leader Board -->
		<section class="section hero is-light">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-full">
						<h2 class="title is-3">OVO-Bench Leaderboard</h2>
					</div>
				</div>
			</div>
		</section>
		<section class="section hero is-centered">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-four-fifths">
						<br>The performance of latest mainstream Video-LLMs on our benchmark, including two 2
						close-sourced models and 6 representative open-source models.
						<br>The best results of each category are highlighted in <b>bold</b>
					</div>
				</div>
				<div class="columns is-centered has-text-centered">
					<div class="column is-full">
						<img src="static/images/leaderboard.png">
					</div>
				</div>
			</div>
		</section>
		<!-- Leader Board -->

		<!-- More Dataset Examples -->
		<section class="section hero is-light">
			<div class="container is-max-desktop">
				<div class="columns is-centered has-text-centered">
					<div class="column is-full">
						<h2 class="title is-3">More Dataset Examples
							<a href="static/examples/benchmark_examples_supplimentary.pdf" download>
								<img src="static/images/pdf_download.png" alt="ä¸‹è½½å›¾æ ‡" class="icon">
							</a>
						</h2>
					</div>
				</div>
			</div>
		</section>
		<!-- Image carousel -->
		<section class="hero is-small">
			<div class="hero-body">
				<div class="container">
					<div id="results-carousel" class="carousel results-carousel">
						<div class="item">
							<!-- Your image here -->
							<img src="static/examples/example_1.png" alt="MY ALT TEXT" />
						</div>
						<div class="item">
							<!-- Your image here -->
							<img src="static/examples/example_2.png" alt="MY ALT TEXT" />
						</div>
						<div class="item">
							<!-- Your image here -->
							<img src="static/examples/example_3.png" alt="MY ALT TEXT" />
						</div>
						<div class="item">
							<!-- Your image here -->
							<img src="static/examples/example_4.png" alt="MY ALT TEXT" />
						</div>
						<div class="item">
							<!-- Your image here -->
							<img src="static/examples/example_5.png" alt="MY ALT TEXT" />
						</div>
						<div class="item">
							<!-- Your image here -->
							<img src="static/examples/example_6.png" alt="MY ALT TEXT" />
						</div>
						<div class="item">
							<!-- Your image here -->
							<img src="static/examples/example_7.png" alt="MY ALT TEXT" />
						</div>
						
					</div>
				</div>
			</div>
		</section>
		<!-- End image carousel -->
		<!-- More Dataset Examples -->

		<!--BibTex citation -->
		<section class="section" id="BibTeX">
			<div class="container is-max-desktop content">
				<h2 class="title">BibTeX</h2>
				<pre><code>
@misc{li2025ovobenchfarvideollmsrealworld,
	title={OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?}, 
	author={Yifei Li and Junbo Niu and Ziyang Miao and Chunjiang Ge and Yuanhang Zhou and Qihao He and Xiaoyi Dong and Haodong Duan and Shuangrui Ding and Rui Qian and Pan Zhang and Yuhang Zang and Yuhang Cao and Conghui He and Jiaqi Wang},
	year={2025},
	eprint={2501.05510},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2501.05510}, 
}
				</code></pre>
			</div>
		</section>
		<!--End BibTex citation -->

		<!-- Statcounter tracking code -->

		<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

		<!-- End of Statcounter Code -->

	</body>
</html>